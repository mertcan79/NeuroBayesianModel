{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "# Add the parent directory to sys.path\n",
    "notebook_dir = os.path.abspath('')\n",
    "project_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from scipy.stats import rv_continuous, rv_discrete\n",
    "\n",
    "\n",
    "from src.data_processing import prepare_data\n",
    "from src.modeling import BayesianModel\n",
    "from src.inference import Inference\n",
    "from src.bayesian_node import BayesianNode, CategoricalNode\n",
    "from src.bayesian_network import BayesianNetwork\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "from src.data_processing import prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Determine environment and data path\n",
    "environment = os.getenv('ENVIRONMENT', 'local')\n",
    "data_path = os.getenv('LOCAL_DATA_PATH') if environment == 'local' else os.getenv('CLOUD_DATA_PATH')\n",
    "\n",
    "# File paths\n",
    "behavioral_path = os.path.join(data_path, 'connectome_behavioral.csv')\n",
    "hcp_path = os.path.join(data_path, 'hcp_freesurfer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_hcp = ['Gender', 'MMSE_Score', 'Age']\n",
    "\n",
    "behavioral_features = [\n",
    "    'Subject', 'Age', 'Gender', 'CogFluidComp_Unadj', 'CogCrystalComp_Unadj', 'MMSE_Score',\n",
    "    'NEOFAC_O', 'NEOFAC_C', 'ProcSpeed_Unadj', 'CardSort_Unadj', 'PicVocab_Unadj', 'ReadEng_Unadj'\n",
    "]\n",
    "\n",
    "hcp_features = [\n",
    "    'Subject', 'FS_TotCort_GM_Vol', 'FS_SubCort_GM_Vol', 'FS_Total_GM_Vol', 'FS_Tot_WM_Vol', 'FS_BrainStem_Vol',\n",
    "    'FS_L_Hippo_Vol', 'FS_R_Hippo_Vol', 'FS_L_Amygdala_Vol', 'FS_R_Amygdala_Vol',\n",
    "    'FS_L_Caudate_Vol', 'FS_R_Caudate_Vol', 'FS_L_Putamen_Vol', 'FS_R_Putamen_Vol',\n",
    "]\n",
    "\n",
    "categorical_columns = ['Gender', 'MMSE_Score', 'Age']\n",
    "\n",
    "prior_edges = [\n",
    "    ('Age', 'CogFluidComp_Unadj'),\n",
    "    ('Age', 'CogCrystalComp_Unadj'),\n",
    "    ('Age', 'MMSE_Score'),\n",
    "    ('Gender', 'CogFluidComp_Unadj'),\n",
    "    ('Gender', 'CogCrystalComp_Unadj'),\n",
    "    ('MMSE_Score', 'CogFluidComp_Unadj'),\n",
    "    ('MMSE_Score', 'CogCrystalComp_Unadj'),\n",
    "    ('FS_Total_GM_Vol', 'CogFluidComp_Unadj'),\n",
    "    ('FS_Total_GM_Vol', 'CogCrystalComp_Unadj'),\n",
    "    ('FS_Tot_WM_Vol', 'CogFluidComp_Unadj'),\n",
    "    ('FS_Tot_WM_Vol', 'CogCrystalComp_Unadj'),\n",
    "    ('FS_L_Hippo_Vol', 'CogFluidComp_Unadj'),\n",
    "    ('FS_R_Hippo_Vol', 'CogFluidComp_Unadj'),\n",
    "    ('FS_L_Amygdala_Vol', 'NEOFAC_O'),\n",
    "    ('FS_R_Amygdala_Vol', 'NEOFAC_O'),\n",
    "    ('NEOFAC_O', 'CogCrystalComp_Unadj'),\n",
    "    ('NEOFAC_C', 'CogFluidComp_Unadj'),\n",
    "    ('FS_L_Hippo_Vol', 'NEOFAC_O'),\n",
    "    ('FS_R_Hippo_Vol', 'NEOFAC_O'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, categorical_columns, categories = prepare_data(\n",
    "    behavioral_path=behavioral_path,\n",
    "    hcp_path=hcp_path,\n",
    "    behavioral_features=behavioral_features,\n",
    "    hcp_features=hcp_features,\n",
    "    categorical_columns=categorical_columns_hcp\n",
    ")\n",
    "\n",
    "data = data.sample(n=100, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMSE_Score unique values: [6 5 4 2 3]\n",
      "MMSE_Score value counts:\n",
      " MMSE_Score\n",
      "5    39\n",
      "6    37\n",
      "4    15\n",
      "3     5\n",
      "2     4\n",
      "Name: count, dtype: int64\n",
      "MMSE_Score dtype: int8\n",
      "MMSE_Score after encoding:\n",
      " MMSE_Score\n",
      "5    39\n",
      "6    37\n",
      "4    15\n",
      "3     5\n",
      "2     4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"MMSE_Score unique values:\", data['MMSE_Score'].unique())\n",
    "print(\"MMSE_Score value counts:\\n\", data['MMSE_Score'].value_counts())\n",
    "print(\"MMSE_Score dtype:\", data['MMSE_Score'].dtype)\n",
    "\n",
    "# If MMSE_Score is categorical, encode it\n",
    "if data['MMSE_Score'].dtype == 'object' or data['MMSE_Score'].dtype == 'category':\n",
    "    data['MMSE_Score'] = pd.Categorical(data['MMSE_Score']).codes\n",
    "\n",
    "print(\"MMSE_Score after encoding:\\n\", data['MMSE_Score'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges learned: [('FS_SubCort_GM_Vol', 'FS_TotCort_GM_Vol'), ('FS_TotCort_GM_Vol', 'FS_SubCort_GM_Vol'), ('FS_TotCort_GM_Vol', 'FS_Total_GM_Vol'), ('FS_TotCort_GM_Vol', 'FS_Tot_WM_Vol'), ('FS_TotCort_GM_Vol', 'FS_BrainStem_Vol'), ('FS_TotCort_GM_Vol', 'FS_L_Hippo_Vol'), ('FS_TotCort_GM_Vol', 'FS_R_Hippo_Vol'), ('FS_TotCort_GM_Vol', 'FS_L_Amygdala_Vol'), ('FS_TotCort_GM_Vol', 'FS_R_Amygdala_Vol'), ('FS_TotCort_GM_Vol', 'FS_L_Caudate_Vol'), ('FS_TotCort_GM_Vol', 'FS_R_Caudate_Vol'), ('FS_TotCort_GM_Vol', 'FS_L_Putamen_Vol'), ('FS_TotCort_GM_Vol', 'FS_R_Putamen_Vol'), ('FS_TotCort_GM_Vol', 'Age'), ('FS_TotCort_GM_Vol', 'Gender'), ('FS_TotCort_GM_Vol', 'CogFluidComp_Unadj'), ('FS_TotCort_GM_Vol', 'CogCrystalComp_Unadj'), ('FS_TotCort_GM_Vol', 'MMSE_Score'), ('FS_TotCort_GM_Vol', 'NEOFAC_O'), ('FS_TotCort_GM_Vol', 'NEOFAC_C'), ('FS_TotCort_GM_Vol', 'ProcSpeed_Unadj'), ('FS_TotCort_GM_Vol', 'CardSort_Unadj'), ('FS_TotCort_GM_Vol', 'PicVocab_Unadj'), ('FS_TotCort_GM_Vol', 'ReadEng_Unadj')]\n",
      "Node: FS_TotCort_GM_Vol\n",
      "  Parents: ['FS_SubCort_GM_Vol']\n",
      "  Children: ['FS_SubCort_GM_Vol', 'FS_Total_GM_Vol', 'FS_Tot_WM_Vol', 'FS_BrainStem_Vol', 'FS_L_Hippo_Vol', 'FS_R_Hippo_Vol', 'FS_L_Amygdala_Vol', 'FS_R_Amygdala_Vol', 'FS_L_Caudate_Vol', 'FS_R_Caudate_Vol', 'FS_L_Putamen_Vol', 'FS_R_Putamen_Vol', 'Age', 'Gender', 'CogFluidComp_Unadj', 'CogCrystalComp_Unadj', 'MMSE_Score', 'NEOFAC_O', 'NEOFAC_C', 'ProcSpeed_Unadj', 'CardSort_Unadj', 'PicVocab_Unadj', 'ReadEng_Unadj']\n",
      "Node: FS_SubCort_GM_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: ['FS_TotCort_GM_Vol']\n",
      "Node: FS_Total_GM_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: FS_Tot_WM_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: FS_BrainStem_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: FS_L_Hippo_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: FS_R_Hippo_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: FS_L_Amygdala_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: FS_R_Amygdala_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: FS_L_Caudate_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: FS_R_Caudate_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: FS_L_Putamen_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: FS_R_Putamen_Vol\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: Age\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: Gender\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: CogFluidComp_Unadj\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: CogCrystalComp_Unadj\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: MMSE_Score\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: NEOFAC_O\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: NEOFAC_C\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: ProcSpeed_Unadj\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: CardSort_Unadj\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: PicVocab_Unadj\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Node: ReadEng_Unadj\n",
      "  Parents: ['FS_TotCort_GM_Vol']\n",
      "  Children: []\n",
      "Parameter initialization complete.\n",
      "Error fitting node MMSE_Score: operands could not be broadcast together with shapes (5,) (7,) \n",
      "Extracted nodes: ['FS_TotCort_GM_Vol', 'FS_SubCort_GM_Vol', 'FS_Total_GM_Vol', 'FS_Tot_WM_Vol', 'FS_BrainStem_Vol', 'FS_L_Hippo_Vol', 'FS_R_Hippo_Vol', 'FS_L_Amygdala_Vol', 'FS_R_Amygdala_Vol', 'FS_L_Caudate_Vol', 'FS_R_Caudate_Vol', 'FS_L_Putamen_Vol', 'FS_R_Putamen_Vol', 'Age', 'Gender', 'CogFluidComp_Unadj', 'CogCrystalComp_Unadj', 'MMSE_Score', 'NEOFAC_O', 'NEOFAC_C', 'ProcSpeed_Unadj', 'CardSort_Unadj', 'PicVocab_Unadj', 'ReadEng_Unadj']\n",
      "Node 'CogFluidComp_Unadj' found in the network.\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Example usage: Compute sensitivity\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     sensitivity \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_sensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_node_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sensitivity)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/NeuroBayesianModel/src/bayesian_network.py:106\u001b[0m, in \u001b[0;36mBayesianNetwork.compute_sensitivity\u001b[0;34m(self, target_node_name, num_samples)\u001b[0m\n\u001b[1;32m    103\u001b[0m inference \u001b[38;5;241m=\u001b[39m Inference(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Sample data for the target node\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m target_samples \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_node_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m sensitivities \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_name, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Documents/NeuroBayesianModel/src/inference.py:29\u001b[0m, in \u001b[0;36mInference.sample_node\u001b[0;34m(self, node_name, size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m distribution:\n\u001b[1;32m     28\u001b[0m     parents \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mparents\n\u001b[0;32m---> 29\u001b[0m     parent_samples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m parent \u001b[38;5;129;01min\u001b[39;00m parents]\n\u001b[1;32m     30\u001b[0m     parent_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack(parent_samples)\n\u001b[1;32m     31\u001b[0m     samples \u001b[38;5;241m=\u001b[39m distribution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintercept\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m parent_values \u001b[38;5;241m@\u001b[39m distribution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, distribution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m], size\u001b[38;5;241m=\u001b[39msize)\n",
      "File \u001b[0;32m~/Documents/NeuroBayesianModel/src/inference.py:29\u001b[0m, in \u001b[0;36mInference.sample_node\u001b[0;34m(self, node_name, size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m distribution:\n\u001b[1;32m     28\u001b[0m     parents \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mparents\n\u001b[0;32m---> 29\u001b[0m     parent_samples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m parent \u001b[38;5;129;01min\u001b[39;00m parents]\n\u001b[1;32m     30\u001b[0m     parent_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack(parent_samples)\n\u001b[1;32m     31\u001b[0m     samples \u001b[38;5;241m=\u001b[39m distribution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintercept\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m parent_values \u001b[38;5;241m@\u001b[39m distribution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, distribution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m], size\u001b[38;5;241m=\u001b[39msize)\n",
      "    \u001b[0;31m[... skipping similar frames: Inference.sample_node at line 29 (2973 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/NeuroBayesianModel/src/inference.py:29\u001b[0m, in \u001b[0;36mInference.sample_node\u001b[0;34m(self, node_name, size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m distribution:\n\u001b[1;32m     28\u001b[0m     parents \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mparents\n\u001b[0;32m---> 29\u001b[0m     parent_samples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m parent \u001b[38;5;129;01min\u001b[39;00m parents]\n\u001b[1;32m     30\u001b[0m     parent_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack(parent_samples)\n\u001b[1;32m     31\u001b[0m     samples \u001b[38;5;241m=\u001b[39m distribution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintercept\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m parent_values \u001b[38;5;241m@\u001b[39m distribution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, distribution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m], size\u001b[38;5;241m=\u001b[39msize)\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "# Initialize and fit the model\n",
    "model = BayesianModel(method='k2', max_parents=4, iterations=100, categorical_columns=categorical_columns)\n",
    "model.fit(data, prior_edges=prior_edges)\n",
    "\n",
    "# Extract nodes from the fitted model\n",
    "nodes = model.network.nodes\n",
    "\n",
    "# Verify node extraction\n",
    "print(\"Extracted nodes:\", list(nodes.keys()))\n",
    "\n",
    "# The BayesianNetwork is already created and fitted within the BayesianModel\n",
    "network = model.network\n",
    "\n",
    "# Verify node existence in the network\n",
    "target_node_name = \"CogFluidComp_Unadj\"\n",
    "if target_node_name in network.nodes:\n",
    "    print(f\"Node '{target_node_name}' found in the network.\")\n",
    "else:\n",
    "    print(f\"Node '{target_node_name}' NOT found in the network.\")\n",
    "    # Print all node names for debugging\n",
    "    print(\"Available nodes in the network:\", list(network.nodes.keys()))\n",
    "\n",
    "# Example usage: Compute sensitivity\n",
    "try:\n",
    "    sensitivity = network.compute_sensitivity(target_node_name)\n",
    "    print(sensitivity)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported distribution type for node CogFluidComp_Unadj: dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sensitivities\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m sensitivity \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_sensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCogFluidComp_Unadj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(sensitivity)\n",
      "File \u001b[0;32m~/Documents/NeuroBayesianModel/src/bayesian_network.py:106\u001b[0m, in \u001b[0;36mBayesianNetwork.compute_sensitivity\u001b[0;34m(self, target_node_name, num_samples)\u001b[0m\n\u001b[1;32m    103\u001b[0m inference \u001b[38;5;241m=\u001b[39m Inference(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Sample data for the target node\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m target_samples \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_node_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m sensitivities \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_name, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Documents/NeuroBayesianModel/src/inference.py:47\u001b[0m, in \u001b[0;36mInference.sample_node\u001b[0;34m(self, node_name, size)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39mmean, scale\u001b[38;5;241m=\u001b[39mstd_dev, size\u001b[38;5;241m=\u001b[39msize)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Add handling for other distribution types as needed\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported distribution type for node \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(distribution)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported distribution type for node CogFluidComp_Unadj: dict"
     ]
    }
   ],
   "source": [
    "# Compute sensitivity using Inference class\n",
    "def compute_sensitivity(network: BayesianNetwork, target_node_name: str, num_samples: int = 1000) -> Dict[str, float]:\n",
    "    if target_node_name not in network.nodes:\n",
    "        raise ValueError(f\"Node {target_node_name} not found in the network.\")\n",
    "    \n",
    "    # Sample data for the target node\n",
    "    target_samples = inference.sample_node(target_node_name, num_samples)\n",
    "    \n",
    "    # Compute sensitivity\n",
    "    sensitivities = {}\n",
    "    for node_name, node in network.nodes.items():\n",
    "        if node_name == target_node_name:\n",
    "            continue\n",
    "        \n",
    "        # Sample for other nodes\n",
    "        other_samples = inference.sample_node(node_name, num_samples)\n",
    "        \n",
    "        # Compute sensitivity (example: mean difference or correlation)\n",
    "        sensitivity = np.mean(target_samples) - np.mean(other_samples)\n",
    "        sensitivities[node_name] = sensitivity\n",
    "    \n",
    "    return sensitivities\n",
    "\n",
    "# Example usage\n",
    "sensitivity = model.network.compute_sensitivity(\"CogFluidComp_Unadj\")\n",
    "print(sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error sampling node: Node CogFluidComp_Unadj not found in the network.\n",
      "Failed to sample node CogFluidComp_Unadj\n"
     ]
    }
   ],
   "source": [
    "def sample_node_with_inference(node_name: str, size: int = 1) -> np.ndarray:\n",
    "    try:\n",
    "        samples = inference.sample_node(node_name, size)\n",
    "        return samples\n",
    "    except Exception as e:\n",
    "        print(f\"Error sampling node: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test sampling a node\n",
    "node_name = 'CogFluidComp_Unadj'\n",
    "samples = sample_node_with_inference(node_name, size=1000)\n",
    "\n",
    "if samples is not None:\n",
    "    print(f\"Samples for {node_name}: {samples[:10]}\")  # Print the first 10 samples\n",
    "else:\n",
    "    print(f\"Failed to sample node {node_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = inference.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Initialization Check\n",
      "Node Name: FS_TotCort_GM_Vol, Type: BayesianNode\n",
      "Node Name: FS_SubCort_GM_Vol, Type: BayesianNode\n",
      "Node Name: FS_Total_GM_Vol, Type: BayesianNode\n",
      "Node Name: FS_Tot_WM_Vol, Type: BayesianNode\n",
      "Node Name: FS_BrainStem_Vol, Type: BayesianNode\n",
      "Node Name: FS_L_Hippo_Vol, Type: BayesianNode\n",
      "Node Name: FS_R_Hippo_Vol, Type: BayesianNode\n",
      "Node Name: FS_L_Amygdala_Vol, Type: BayesianNode\n",
      "Node Name: FS_R_Amygdala_Vol, Type: BayesianNode\n",
      "Node Name: FS_L_Caudate_Vol, Type: BayesianNode\n",
      "Node Name: FS_R_Caudate_Vol, Type: BayesianNode\n",
      "Node Name: FS_L_Putamen_Vol, Type: BayesianNode\n",
      "Node Name: FS_R_Putamen_Vol, Type: BayesianNode\n",
      "Node Name: Age, Type: BayesianNode\n",
      "Node Name: Gender, Type: BayesianNode\n",
      "Node Name: CogFluidComp_Unadj, Type: BayesianNode\n",
      "Node Name: CogCrystalComp_Unadj, Type: BayesianNode\n",
      "Node Name: MMSE_Score, Type: BayesianNode\n",
      "Node Name: NEOFAC_O, Type: BayesianNode\n",
      "Node Name: NEOFAC_C, Type: BayesianNode\n",
      "Node Name: ProcSpeed_Unadj, Type: BayesianNode\n",
      "Node Name: CardSort_Unadj, Type: BayesianNode\n",
      "Node Name: PicVocab_Unadj, Type: BayesianNode\n",
      "Node Name: ReadEng_Unadj, Type: BayesianNode\n",
      "\n",
      "Distributions Check\n",
      "Error with node FS_TotCort_GM_Vol: name 'stats' is not defined\n",
      "Error with node FS_SubCort_GM_Vol: name 'stats' is not defined\n",
      "Error with node FS_Total_GM_Vol: name 'stats' is not defined\n",
      "Error with node FS_Tot_WM_Vol: name 'stats' is not defined\n",
      "Error with node FS_BrainStem_Vol: name 'stats' is not defined\n",
      "Error with node FS_L_Hippo_Vol: name 'stats' is not defined\n",
      "Error with node FS_R_Hippo_Vol: name 'stats' is not defined\n",
      "Error with node FS_L_Amygdala_Vol: name 'stats' is not defined\n",
      "Error with node FS_R_Amygdala_Vol: name 'stats' is not defined\n",
      "Error with node FS_L_Caudate_Vol: name 'stats' is not defined\n",
      "Error with node FS_R_Caudate_Vol: name 'stats' is not defined\n",
      "Error with node FS_L_Putamen_Vol: name 'stats' is not defined\n",
      "Error with node FS_R_Putamen_Vol: name 'stats' is not defined\n",
      "Error with node Age: name 'stats' is not defined\n",
      "Error with node Gender: name 'stats' is not defined\n",
      "Error with node CogFluidComp_Unadj: name 'stats' is not defined\n",
      "Error with node CogCrystalComp_Unadj: name 'stats' is not defined\n",
      "Error with node MMSE_Score: name 'stats' is not defined\n",
      "Error with node NEOFAC_O: name 'stats' is not defined\n",
      "Error with node NEOFAC_C: name 'stats' is not defined\n",
      "Error with node ProcSpeed_Unadj: name 'stats' is not defined\n",
      "Error with node CardSort_Unadj: name 'stats' is not defined\n",
      "Error with node PicVocab_Unadj: name 'stats' is not defined\n",
      "Error with node ReadEng_Unadj: name 'stats' is not defined\n",
      "\n",
      "Network Structure Check\n",
      "\n",
      "Inference Test\n",
      "Samples for CogFluidComp_Unadj: [ 0.95523927  0.40232437  0.48835562  1.86438911  0.64413196 -0.30130443\n",
      " -0.78681586 -0.39646575 -0.19437846 -0.50648209]\n",
      "Sensitivity for CogFluidComp_Unadj: {'FS_TotCort_GM_Vol': 0.09011243338323295, 'FS_SubCort_GM_Vol': 0.05442333326431852, 'FS_Total_GM_Vol': 0.12756956069562916, 'FS_Tot_WM_Vol': 0.06277465999327103, 'FS_BrainStem_Vol': -0.03604019054635843, 'FS_L_Hippo_Vol': -0.010306111598529052, 'FS_R_Hippo_Vol': 0.022424127586708503, 'FS_L_Amygdala_Vol': 0.0004260983817248182, 'FS_R_Amygdala_Vol': 0.11792165124831216, 'FS_L_Caudate_Vol': 0.07263501272709395, 'FS_R_Caudate_Vol': 0.08312969918980127, 'FS_L_Putamen_Vol': 0.13392170453581292, 'FS_R_Putamen_Vol': 0.06734544874776627, 'Age': -1.083827350858431, 'Gender': -0.36906064100083985, 'CogCrystalComp_Unadj': -0.0718609441612437, 'MMSE_Score': -28.996360517428815, 'NEOFAC_O': 0.02790227403910253, 'NEOFAC_C': 0.09373747695818177, 'ProcSpeed_Unadj': 0.007884637537740838, 'CardSort_Unadj': -0.04699190208987196, 'PicVocab_Unadj': 0.042536299716846374, 'ReadEng_Unadj': -0.07723950255695022}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Verify Node Initialization\n",
    "print(\"Node Initialization Check\")\n",
    "for node_name, node in nodes.items():\n",
    "    if isinstance(node, BayesianNode):\n",
    "        print(f\"Node Name: {node_name}, Type: BayesianNode\")\n",
    "    else:\n",
    "        print(f\"Node Name: {node_name}, Type: {type(node).__name__}\")\n",
    "\n",
    "# 2. Check Distributions for Nodes\n",
    "print(\"\\nDistributions Check\")\n",
    "for node_name, node in nodes.items():\n",
    "    try:\n",
    "        distribution = node.get_distribution()\n",
    "        if isinstance(distribution, (stats.rv_continuous, stats.rv_discrete)):\n",
    "            print(f\"Node Name: {node_name}\")\n",
    "            print(f\"Distribution: {distribution}\")\n",
    "            print(f\"Distribution Type: {type(distribution).__name__}\")\n",
    "            samples = distribution.rvs(size=10)\n",
    "            print(f\"Samples: {samples}\")\n",
    "        else:\n",
    "            print(f\"Node {node_name} has an unsupported distribution type: {type(distribution).__name__}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with node {node_name}: {e}\")\n",
    "\n",
    "# 3. Verify Network Structure\n",
    "print(\"\\nNetwork Structure Check\")\n",
    "try:\n",
    "    # Check if network structure is properly defined\n",
    "    for node_name, node in nodes.items():\n",
    "        if not hasattr(node, 'children'):\n",
    "            print(f\"Node {node_name} is missing 'children' attribute.\")\n",
    "except AttributeError as e:\n",
    "    print(f\"Network Structure Error: {e}\")\n",
    "\n",
    "# 4. Test Inference Class\n",
    "print(\"\\nInference Test\")\n",
    "try:\n",
    "    # Test sampling from a node\n",
    "    node_name = 'CogFluidComp_Unadj'\n",
    "    try:\n",
    "        samples = inference.sample_node(node_name, size=10)\n",
    "        print(f\"Samples for {node_name}: {samples}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"Sampling Error: {ve}\")\n",
    "    \n",
    "    # Test sensitivity computation (assuming compute_sensitivity function exists)\n",
    "    try:\n",
    "        sensitivity = compute_sensitivity(network, node_name)  # Ensure 'network' is defined\n",
    "        print(f\"Sensitivity for {node_name}: {sensitivity}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"Sensitivity Computation Error: {ve}\")\n",
    "except Exception as e:\n",
    "    print(f\"Inference Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
